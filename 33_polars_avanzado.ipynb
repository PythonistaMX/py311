{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polars Avanzado\n",
    "\n",
    "En este notebook exploraremos características avanzadas de Polars que te permitirán escribir código de análisis de datos profesional, eficiente y escalable:\n",
    "\n",
    "* **Lazy Evaluation profunda:** Optimización de consultas\n",
    "* **Window Functions:** Operaciones analíticas complejas\n",
    "* **Joins optimizados:** Fusión eficiente de DataFrames\n",
    "* **Pivoting y Crosstabs:** Transformación de datos\n",
    "* **I/O de alto rendimiento:** Lectura/escritura optimizada\n",
    "* **Decisiones arquitectónicas:** Cuándo usar Polars, Pandas o Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "print(f\"Polars {pl.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lazy Evaluation Profunda\n",
    "\n",
    "La **evaluación lazy** es el corazón de la eficiencia de Polars. Las operaciones no se ejecutan inmediatamente, sino que se construye un plan de ejecución optimizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear datos de ejemplo\n",
    "df = pl.DataFrame({\n",
    "    'id': range(1, 101),\n",
    "    'nombre': [f'persona_{i}' for i in range(100)],\n",
    "    'departamento': ['Ventas', 'IT', 'HR', 'Finanzas'] * 25,\n",
    "    'salario': np.random.randint(40000, 120000, 100),\n",
    "    'fecha_contrato': [datetime(2020, 1, 1) + timedelta(days=x) for x in range(100)]\n",
    "})\n",
    "\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construcción de query lazy\n",
    "query = (df\n",
    "    .lazy()\n",
    "    .filter(pl.col('salario') > 60000)\n",
    "    .select(['nombre', 'departamento', 'salario'])\n",
    "    .sort('salario', descending=True)\n",
    ")\n",
    "\n",
    "# Aún no se ejecutó nada\n",
    "print(f\"Tipo: {type(query)}\")\n",
    "print(f\"Es lazy: {isinstance(query, pl.LazyFrame)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver el plan de ejecución (antes de optimize)\n",
    "print(\"Plan sin optimizar:\")\n",
    "print(query.explain(optimized=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver el plan optimizado\n",
    "print(\"Plan optimizado (lo que realmente ejecuta):\")\n",
    "print(query.explain(optimized=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar la query\n",
    "result = query.collect()\n",
    "print(f\"Resultado ({len(result)} filas):\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beneficios de Lazy Evaluation:\n",
    "\n",
    "1. **Optimización automática:** Predicados push-down, eliminación de columnas no usadas\n",
    "2. **Memoria eficiente:** Solo materializa lo necesario\n",
    "3. **Paralelización:** Automática en múltiples núcleos\n",
    "4. **Simplificación:** Código más legible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window Functions\n",
    "\n",
    "Las **funciones de ventana (window functions)** permiten cálculos analíticos complejos sobre grupos de filas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear datos de ventas\n",
    "ventas = pl.DataFrame({\n",
    "    'fecha': ['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04',\n",
    "              '2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04'] * 3,\n",
    "    'producto': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B',\n",
    "                 'C', 'C', 'C', 'C', 'A', 'A', 'A', 'A',\n",
    "                 'B', 'B', 'B', 'B', 'C', 'C', 'C', 'C'],\n",
    "    'monto': [100, 150, 120, 180, 200, 250, 300, 280,\n",
    "              75, 95, 110, 130, 110, 160, 130, 190,\n",
    "              210, 260, 310, 290, 85, 105, 120, 140]\n",
    "})\n",
    "\n",
    "print(f\"Ventas shape: {ventas.shape}\")\n",
    "print(ventas.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window function: suma acumulada por producto\n",
    "resultado = ventas.with_columns(\n",
    "    pl.col('monto').cum_sum().over('producto').alias('suma_acumulada')\n",
    ")\n",
    "\n",
    "print(\"Con suma acumulada por producto:\")\n",
    "print(resultado.sort(['producto', 'fecha']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window functions: rank y row_number\n",
    "resultado = ventas.with_columns(\n",
    "    pl.col('monto').rank(method='dense').over('producto').alias('rank_monto'),\n",
    "    pl.col('monto').rank(method='ordinal').over('producto').alias('row_number')\n",
    ")\n",
    "\n",
    "print(\"Con ranking por producto:\")\n",
    "print(resultado.sort(['producto', 'monto'], descending=[False, True]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lag y Lead: acceder a filas anteriores/siguientes\n",
    "resultado = ventas.with_columns(\n",
    "    pl.col('monto').lag().over('producto').alias('monto_anterior'),\n",
    "    pl.col('monto').lead().over('producto').alias('monto_siguiente')\n",
    ")\n",
    "\n",
    "print(\"Con lag/lead por producto:\")\n",
    "print(resultado.sort(['producto', 'fecha']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joins Optimizados\n",
    "\n",
    "Polars realiza joins de forma muy eficiente, especialmente con evaluación lazy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear dos DataFrames para unir\n",
    "empleados = pl.DataFrame({\n",
    "    'empleado_id': [1, 2, 3, 4, 5],\n",
    "    'nombre': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n",
    "    'departamento': ['Ventas', 'IT', 'HR', 'Ventas', 'IT']\n",
    "})\n",
    "\n",
    "salarios = pl.DataFrame({\n",
    "    'empleado_id': [1, 2, 3, 4, 5],\n",
    "    'salario': [50000, 80000, 45000, 55000, 90000],\n",
    "    'bonus': [5000, 8000, 0, 5500, 9000]\n",
    "})\n",
    "\n",
    "print(\"Empleados:\")\n",
    "print(empleados)\n",
    "print(\"\\nSalarios:\")\n",
    "print(salarios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner join\n",
    "resultado_inner = empleados.join(\n",
    "    salarios,\n",
    "    on='empleado_id',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(\"Inner Join:\")\n",
    "print(resultado_inner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left join con múltiples columnas de join\n",
    "# Crear datos con más columnas de unión\n",
    "tabla_a = pl.DataFrame({\n",
    "    'ano': [2023, 2023, 2024, 2024],\n",
    "    'trimestre': [1, 2, 1, 2],\n",
    "    'ventas': [100, 150, 120, 180]\n",
    "})\n",
    "\n",
    "tabla_b = pl.DataFrame({\n",
    "    'ano': [2023, 2023, 2024, 2024],\n",
    "    'trimestre': [1, 2, 1, 2],\n",
    "    'gastos': [60, 80, 70, 100]\n",
    "})\n",
    "\n",
    "resultado = tabla_a.join(\n",
    "    tabla_b,\n",
    "    on=['ano', 'trimestre'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(\"Join en múltiples columnas:\")\n",
    "print(resultado)\n",
    "print(\"\\nCon columna calculada:\")\n",
    "resultado = resultado.with_columns(\n",
    "    (pl.col('ventas') - pl.col('gastos')).alias('ganancia')\n",
    ")\n",
    "print(resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivoting y Crosstabs\n",
    "\n",
    "Transformar datos de formato largo a ancho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear datos en formato largo\n",
    "datos_largo = pl.DataFrame({\n",
    "    'ano': [2023, 2023, 2023, 2023, 2024, 2024, 2024, 2024],\n",
    "    'trimestre': ['Q1', 'Q2', 'Q3', 'Q4', 'Q1', 'Q2', 'Q3', 'Q4'],\n",
    "    'metrica': ['ventas', 'ventas', 'ventas', 'ventas', 'ventas', 'ventas', 'ventas', 'ventas'],\n",
    "    'departamento': ['A', 'A', 'B', 'B', 'A', 'A', 'B', 'B'],\n",
    "    'valor': [100, 150, 200, 180, 120, 160, 210, 190]\n",
    "})\n",
    "\n",
    "print(\"Datos en formato largo:\")\n",
    "print(datos_largo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot: convertir a formato ancho\n",
    "datos_ancho = datos_largo.pivot(\n",
    "    index=['ano', 'departamento'],\n",
    "    columns='trimestre',\n",
    "    values='valor',\n",
    "    aggregate_function='first'\n",
    ")\n",
    "\n",
    "print(\"Datos en formato ancho:\")\n",
    "print(datos_ancho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpivot: convertir de ancho a largo\n",
    "datos_reconvertidos = datos_ancho.unpivot(\n",
    "    index=['ano', 'departamento'],\n",
    "    variable_name='trimestre',\n",
    "    value_name='valor'\n",
    ")\n",
    "\n",
    "print(\"Reconvertido a formato largo:\")\n",
    "print(datos_reconvertidos.sort(['ano', 'departamento', 'trimestre']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I/O de Alto Rendimiento\n",
    "\n",
    "Polars es muy eficiente en la lectura y escritura de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear datos de ejemplo\n",
    "datos_grandes = pl.DataFrame({\n",
    "    'id': range(10000),\n",
    "    'valor': np.random.rand(10000),\n",
    "    'categoria': np.random.choice(['A', 'B', 'C'], 10000)\n",
    "})\n",
    "\n",
    "# Escribir en Parquet con compresión\n",
    "datos_grandes.write_parquet('datos_optimizado.parquet', compression='zstd')\n",
    "print(\"✓ Guardado en Parquet con compresión zstd\")\n",
    "\n",
    "# Leer de forma lazy (solo metadatos)\n",
    "df_lazy = pl.scan_parquet('datos_optimizado.parquet')\n",
    "print(f\"\\nLazyFrame escaneado: {type(df_lazy)}\")\n",
    "\n",
    "# Ejecutar con filtro (solo se leen las filas necesarias)\n",
    "resultado = df_lazy.filter(pl.col('valor') > 0.9).collect()\n",
    "print(f\"Filas con valor > 0.9: {len(resultado)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lectura particionada (para archivos grandes divididos en carpetas)\n",
    "# Guardar con particiones\n",
    "datos_grandes.write_parquet(\n",
    "    'datos_particionados/',\n",
    "    partition_by='categoria'\n",
    ")\n",
    "print(\"✓ Guardado en Parquet particionado por categoría\")\n",
    "\n",
    "# Leer datos particionados\n",
    "df_particionado = pl.scan_parquet('datos_particionados/*.parquet')\n",
    "resultado = df_particionado.filter(pl.col('categoria') == 'A').collect()\n",
    "print(f\"Filas en categoría A: {len(resultado)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decisiones Arquitectónicas: Polars vs Pandas vs Dask\n",
    "\n",
    "### Matriz de Decisión\n",
    "\n",
    "| Situación | Recomendación | Razón |\n",
    "| :--- | :--- | :--- |\n",
    "| Datos < 1GB | **Pandas** | Suficientemente rápido, ecosistema amplio |\n",
    "| Datos 1-20GB | **Polars** | Rápido, eficiente, API moderna |\n",
    "| Datos > 20GB | **Dask** | Distribuido, puede escalar |\n",
    "| Análisis interactivo | **Pandas** | Mejor para exploración |\n",
    "| Pipelines ETL | **Polars** | API clara, evaluación lazy |\n",
    "| Computación distribuida | **Dask** | Paralelización automática |\n",
    "| Necesidad de GPU | **Especial** | Considerar RAPIDS, CuDF |\n",
    "| Compatibilidad máxima | **Pandas** | Más librerías lo soportan |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparativa de tiempos\n",
    "import time\n",
    "\n",
    "# Crear datos medianos\n",
    "n = 5_000_000\n",
    "datos = {\n",
    "    'id': range(n),\n",
    "    'valor': np.random.rand(n),\n",
    "    'categoria': np.random.choice(['A', 'B', 'C', 'D'], n)\n",
    "}\n",
    "\n",
    "# Benchmark Pandas\n",
    "start = time.time()\n",
    "df_pandas = pd.DataFrame(datos)\n",
    "resultado_pandas = df_pandas[df_pandas['valor'] > 0.5].groupby('categoria')['valor'].mean()\n",
    "tiempo_pandas = time.time() - start\n",
    "\n",
    "# Benchmark Polars\n",
    "start = time.time()\n",
    "df_polars = pl.DataFrame(datos)\n",
    "resultado_polars = df_polars.filter(pl.col('valor') > 0.5).groupby('categoria').agg(pl.col('valor').mean())\n",
    "tiempo_polars = time.time() - start\n",
    "\n",
    "print(f\"Pandas: {tiempo_pandas:.4f}s\")\n",
    "print(f\"Polars: {tiempo_polars:.4f}s\")\n",
    "print(f\"Polars es {tiempo_pandas/tiempo_polars:.1f}x más rápido\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Casos de Uso Reales\n",
    "\n",
    "### Caso 1: ETL de Datos Grandes\n",
    "\n",
    "**Escenario:** Procesar archivos Parquet de 10GB diarios\n",
    "\n",
    "**Solución Polars:**\n",
    "```python\n",
    "df = pl.scan_parquet('data/**/*.parquet')\n",
    "result = (df\n",
    "    .filter(pl.col('date') >= '2024-01-01')\n",
    "    .groupby('category').agg(pl.col('amount').sum())\n",
    "    .sort('amount', descending=True)\n",
    "    .collect()\n",
    ")\n",
    "result.write_parquet('output.parquet')\n",
    "```\n",
    "\n",
    "### Caso 2: Dashboard en Vivo\n",
    "\n",
    "**Escenario:** Datos que cambian constantemente, necesitas resp rápidas\n",
    "\n",
    "**Solución Polars:**\n",
    "- Carga datos en memoria con Polars\n",
    "- Ejecuta consultas lazy para máxima velocidad\n",
    "- Refrescar cada N segundos\n",
    "\n",
    "### Caso 3: Análisis Exploratorio\n",
    "\n",
    "**Escenario:** Exploración interactiva de datos\n",
    "\n",
    "**Recomendación:** Pandas + Jupyter\n",
    "- Mejor para análisis ad-hoc\n",
    "- Más visualizaciones disponibles\n",
    "- Transición a Polars si es lento\n",
    "\n",
    "### Caso 4: Big Data Distribuido\n",
    "\n",
    "**Escenario:** Cluster con múltiples máquinas, TB de datos\n",
    "\n",
    "**Solución Dask:**\n",
    "```python\n",
    "ddf = dd.read_parquet('s3://bucket/data/')\n",
    "result = ddf[ddf['value'] > 100].groupby('category').mean().compute()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen y Conclusiones\n",
    "\n",
    "### Características Clave de Polars:\n",
    "\n",
    "✓ **Velocidad:** 5-10x más rápido que Pandas\n",
    "\n",
    "✓ **Memoria:** Mejor gestión de RAM\n",
    "\n",
    "✓ **Lazy Evaluation:** Optimización automática\n",
    "\n",
    "✓ **API moderna:** Expresiones composables\n",
    "\n",
    "✓ **Tipos ricos:** Mejor manejo de datos complejos\n",
    "\n",
    "### Cuando Usar Cada Herramienta:\n",
    "\n",
    "| Herramienta | Mejor Para | No Usar Para |\n",
    "| :--- | :--- | :--- |\n",
    "| **Pandas** | Análisis interactivo, <1GB | Datos muy grandes |\n",
    "| **Polars** | ETL, 1-20GB, velocidad | Cuando Pandas es suficiente |\n",
    "| **Dask** | Big Data distribuido, >20GB | Datos pequeños (overhead) |\n",
    "\n",
    "### Próximos Pasos:\n",
    "\n",
    "1. Practica migrando código Pandas a Polars\n",
    "2. Mide performance en tus datasets\n",
    "3. Explora lazy evaluation\n",
    "4. Aprende sobre optimizaciones específicas del dominio\n",
    "\n",
    "¡Ya estás listo para usar Polars en producción!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}